<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">

<channel>
  <title>Top Scirate Papers</title>
  <link>https://www.scirate.com</link>
  <description>The feed lists the top "scited" papers on the scirate website, often featuring the most widely appreciated quantum physics (quant-ph) preprints.</description>

  <item>
    <title>Error Correction of Quantum Algorithms: Arbitrarily Accurate Recovery Of Noisy Quantum Signal Processing</title>
    <link>http://arxiv.org/pdf/2301.08542</link>
    <author>Andrew K. Tan, Yuan Liu, Minh C. Tran, Isaac L. Chuang</author>
    <pubDate>Jan 23 2023</pubDate>
    <description>The intrinsic probabilistic nature of quantum systems makes error correction or mitigation indispensable for quantum computation. While current error-correcting strategies focus on correcting errors in quantum states or quantum gates, these fine-grained error-correction methods can incur significant overhead for quantum algorithms of increasing complexity. We present a first step in achieving error correction at the level of quantum algorithms by combining a unified perspective on modern quantum algorithms via quantum signal processing (QSP). An error model of under- or over-rotation of the signal processing operator parameterized by $\epsilon < 1$ is introduced. It is shown that while Pauli $Z$-errors are not recoverable without additional resources, Pauli $X$ and $Y$ errors can be arbitrarily suppressed by coherently appending a noisy `recovery QSP.' Furthermore, it is found that a recovery QSP of length $O(2^k c^{k^2} d)$ is sufficient to correct any length-$d$ QSP with $c$ unique phases to $k^{th}$-order in error $\epsilon$. Allowing an additional assumption, a lower bound of $\Omega(cd)$ is shown, which is tight for $k = 1$, on the length of the recovery sequence. Our algorithmic-level error correction method is applied to Grover's fixed-point search algorithm as a demonstration.</description>
  </item>

  <item>
    <title>Entropy Uncertainty Relations and Strong Sub-additivity of Quantum Channels</title>
    <link>http://arxiv.org/pdf/2301.08402</link>
    <author>Li Gao, Marius Junge, Nicholas LaRacuente</author>
    <pubDate>Jan 23 2023</pubDate>
    <description>We prove an entropic uncertainty relation for two quantum channels, extending the work of Frank and Lieb for quantum measurements. This is obtained via a generalized strong super-additivity (SSA) of quantum entropy. Motivated by Petz's algebraic SSA inequality, we also obtain a generalized SSA for quantum relative entropy. As a special case, it gives an improved data processing inequality.</description>
  </item>

  <item>
    <title>Multi armed bandits and quantum channel oracles</title>
    <link>http://arxiv.org/pdf/2301.08544</link>
    <author>Simon Buchholz, Jonas M. Kübler, Bernhard Schölkopf</author>
    <pubDate>Jan 23 2023</pubDate>
    <description>Multi armed bandits are one of the theoretical pillars of reinforcement learning. Recently, the investigation of quantum algorithms for multi armed bandit problems was started, and it was found that a quadratic speed-up is possible when the arms and the randomness of the rewards of the arms can be queried in superposition. Here we introduce further bandit models where we only have limited access to the randomness of the rewards, but we can still query the arms in superposition. We show that this impedes any speed-up of quantum algorithms.</description>
  </item>

  <item>
    <title>Superpolynomial Lower Bounds for Learning Monotone Classes</title>
    <link>http://arxiv.org/pdf/2301.08486</link>
    <author>Nader H. Bshouty</author>
    <pubDate>Jan 23 2023</pubDate>
    <description>Koch, Strassle, and Tan [SODA 2023], show that, under the randomized exponential time hypothesis, there is no distribution-free PAC-learning algorithm that runs in time $n^{\tilde O(\log\log s)}$ for the classes of $n$-variable size-$s$ DNF, size-$s$ Decision Tree, and $\log s$-Junta by DNF (that returns a DNF hypothesis). Assuming a natural conjecture on the hardness of set cover, they give the lower bound $n^{\Omega(\log s)}$. This matches the best known upper bound for $n$-variable size-$s$ Decision Tree, and $\log s$-Junta. In this paper, we give the same lower bounds for PAC-learning of $n$-variable size-$s$ Monotone DNF, size-$s$ Monotone Decision Tree, and Monotone $\log s$-Junta by~DNF. This solves the open problem proposed by Koch, Strassle, and Tan and subsumes the above results. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time, and can compute the target function on all the points of the support of the distribution in polynomial time.</description>
  </item>

  <item>
    <title>FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer</title>
    <link>http://arxiv.org/pdf/2301.08739</link>
    <author>Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, Song Han</author>
    <pubDate>Jan 23 2023</pubDate>
    <description>Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3x slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving). This inefficiency comes from point clouds' sparse and irregular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better computational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effectively avoids expensive structuring and padding overheads. We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from different directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup over (sparse convolutional) CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks. Code to reproduce our results will be made publicly available.</description>
  </item>

</channel>

</rss>