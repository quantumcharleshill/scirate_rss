<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">

<channel>
  <title>Top Scirate Papers</title>
  <link>https://www.scirate.com</link>
  <description>The feed lists the top "scited" papers on the scirate website, often featuring the most widely appreciated quantum physics (quant-ph) preprints.</description>

  <item>
    <title>Quantum Routing with Teleportation</title>
    <link>http://arxiv.org/pdf/2204.04185</link>
    <author>Dhruv Devulapalli, Eddie Schoute, Aniruddha Bapat, Andrew M. Childs, Alexey V. Gorshkov</author>
    <pubDate>Apr 11 2022</pubDate>
    <description>We study the problem of implementing arbitrary permutations of qubits under interaction constraints in quantum systems that allow for arbitrarily fast local operations and classical communication (LOCC). In particular, we show examples of speedups over swap-based and more general unitary routing methods by distributing entanglement and using LOCC to perform quantum teleportation. We further describe an example of an interaction graph for which teleportation gives a logarithmic speedup in the worst-case routing time over swap-based routing. We also study limits on the speedup afforded by quantum teleportation - showing an $O(\sqrt{N \log N})$ upper bound on the separation in routing time for any interaction graph - and give tighter bounds for some common classes of graphs.</description>
  </item>

  <item>
    <title>Modern applications of machine learning in quantum sciences</title>
    <link>http://arxiv.org/pdf/2204.04198</link>
    <author>Anna Dawid, Julian Arnold, Borja Requena, Alexander Gresch, Marcin Płodzień, Kaelan Donatella, Kim Nicoli, Paolo Stornati, Rouven Koch, Miriam Büttner, Robert Okuła, Gorka Muñoz-Gil, Rodrigo A. Vargas-Hernández, Alba Cervera-Lierta, Juan Carrasquilla, Vedran Dunjko, Marylou Gabrié, Patrick Huembeli, Evert van Nieuwenburg, Filippo Vicentini, et al (9)</author>
    <pubDate>Apr 11 2022</pubDate>
    <description>In these Lecture Notes, we provide a comprehensive introduction to the most recent advances in the application of machine learning methods in quantum sciences. We cover the use of deep learning and kernel methods in supervised, unsupervised, and reinforcement learning algorithms for phase classification, representation of many-body quantum states, quantum feedback control, and quantum circuits optimization. Moreover, we introduce and discuss more specialized topics such as differentiable programming, generative models, statistical approach to machine learning, and quantum machine learning.</description>
  </item>

  <item>
    <title>Matrix multiplication via matrix groups</title>
    <link>http://arxiv.org/pdf/2204.03826</link>
    <author>Jonah Blasiak, Henry Cohn, Joshua A. Grochow, Kevin Pratt, Chris Umans</author>
    <pubDate>Apr 11 2022</pubDate>
    <description>In 2003, Cohn and Umans proposed a group-theoretic approach to bounding the exponent of matrix multiplication. Previous work within this approach ruled out certain families of groups as a route to obtaining $\omega = 2$, while other families of groups remain potentially viable. In this paper we turn our attention to matrix groups, whose usefulness within this framework was relatively unexplored. We first show that groups of Lie type cannot prove $\omega=2$ within the group-theoretic approach. This is based on a representation-theoretic argument that identifies the second-smallest dimension of an irreducible representation of a group as a key parameter that determines its viability in this framework. Our proof builds on Gowers' result concerning product-free sets in quasirandom groups. We then give another barrier that rules out certain natural matrix group constructions that make use of subgroups that are far from being self-normalizing. Our barrier results leave open several natural paths to obtain $\omega = 2$ via matrix groups. To explore these routes we propose working in the continuous setting of Lie groups, in which we develop an analogous theory. Obtaining the analogue of $\omega=2$ in this potentially easier setting is a key challenge that represents an intermediate goal short of actually proving $\omega = 2$. We give two constructions in the continuous setting, each of which evades one of our two barriers.</description>
  </item>

  <item>
    <title>Learning Polynomial Transformations</title>
    <link>http://arxiv.org/pdf/2204.04209</link>
    <author>Sitan Chen, Jerry Li, Yuanzhi Li, Anru R. Zhang</author>
    <pubDate>Apr 11 2022</pubDate>
    <description>We consider the problem of learning high dimensional polynomial transformations of Gaussians. Given samples of the form $p(x)$, where $x\sim N(0, \mathrm{Id}_r)$ is hidden and $p: \mathbb{R}^r \to \mathbb{R}^d$ is a function where every output coordinate is a low-degree polynomial, the goal is to learn the distribution over $p(x)$. This problem is natural in its own right, but is also an important special case of learning deep generative models, namely pushforwards of Gaussians under two-layer neural networks with polynomial activations. Understanding the learnability of such generative models is crucial to understanding why they perform so well in practice. Our first main result is a polynomial-time algorithm for learning quadratic transformations of Gaussians in a smoothed setting. Our second main result is a polynomial-time algorithm for learning constant-degree polynomial transformations of Gaussian in a smoothed setting, when the rank of the associated tensors is small. In fact our results extend to any rotation-invariant input distribution, not just Gaussian. These are the first end-to-end guarantees for learning a pushforward under a neural network with more than one layer. Along the way, we also give the first polynomial-time algorithms with provable guarantees for tensor ring decomposition, a popular generalization of tensor decomposition that is used in practice to implicitly store large tensors.</description>
  </item>

  <item>
    <title>Finding a Winning Strategy for Wordle is NP-complete</title>
    <link>http://arxiv.org/pdf/2204.04104</link>
    <author>Will Rosenbaum</author>
    <pubDate>Apr 11 2022</pubDate>
    <description>In this paper, we give a formal definition of the popular word-guessing game Wordle. We show that, in general, determining if a given Wordle instance admits a winning strategy is NP-complete. We also show that given a Wordle instance of size $N$, a winning strategy that uses $g$ guesses in the worst case (if any) can be found in time $N^{O(g)}$.</description>
  </item>

</channel>

</rss>