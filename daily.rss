<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">

<channel>
  <title>Top Scirate Papers</title>
  <link>https://www.scirate.com</link>
  <description>The feed lists the top "scited" papers on the scirate website, often featuring the most widely appreciated quantum physics (quant-ph) preprints.</description>

  <item>
    <title>Measuring magic on a quantum processor</title>
    <link>http://arxiv.org/pdf/2204.00015</link>
    <author>Salvatore F.E. Oliviero, Lorenzo Leone, Alioscia Hamma, Seth Lloyd</author>
    <pubDate>Apr 04 2022</pubDate>
    <description>Magic states are the resource that allows quantum computers to attain an advantage over classical computers. This resource consists in the deviation from a property called stabilizerness which in turn implies that stabilizer circuits can be efficiently simulated on a classical computer. Without magic, no quantum computer can do anything that a classical computer cannot do. Given the importance of magic for quantum computation, it would be useful to have a method for measuring the amount of magic in a quantum state. In this work, we propose and experimentally demonstrate a protocol for measuring magic based on randomized measurements. Our experiments are carried out on two IBM Quantum Falcon processors. This protocol can provide a characterization of the effectiveness of a quantum hardware in producing states that cannot be effectively simulated on a classical computer. We show how from these measurements one can construct realistic noise models affecting the hardware.</description>
  </item>

  <item>
    <title>Constructing all qutrit controlled Clifford+T gates in Clifford+T</title>
    <link>http://arxiv.org/pdf/2204.00552</link>
    <author>Lia Yeh, John van de Wetering</author>
    <pubDate>Apr 04 2022</pubDate>
    <description>For a number of useful quantum circuits, qudit constructions have been found which reduce resource requirements compared to the best known or best possible qubit construction. However, many of the necessary qutrit gates in these constructions have never been explicitly and efficiently constructed in a fault-tolerant manner. We show how to exactly and unitarily construct any qutrit multiple-controlled Clifford+T unitary using just Clifford+T gates and without using ancillae. The T-count to do so is polynomial in the number of controls $k$, scaling as $O(k^{3.585})$. With our results we can construct ancilla-free Clifford+T implementations of multiple-controlled T gates as well as all versions of the qutrit multiple-controlled Toffoli, while the analogous results for qubits are impossible. As an application of our results, we provide a procedure to implement any ternary classical reversible function on $n$ trits as an ancilla-free qutrit unitary using $O(3^n n^{3.585})$ T gates.</description>
  </item>

  <item>
    <title>Perceval: A Software Platform for Discrete Variable Photonic Quantum Computing</title>
    <link>http://arxiv.org/pdf/2204.00602</link>
    <author>Nicolas Heurtel, Andreas Fyrillas, Grégoire de Gliniasty, Raphaël Le Bihan, Sébastien Malherbe, Marceau Pailhas, Boris Bourdoncle, Pierre-Emmanuel Emeriau, Rawad Mezher, Luka Music, Nadia Belabas, Benoît Valiron, Pascale Senellart, Shane Mansfield, Jean Senellart</author>
    <pubDate>Apr 04 2022</pubDate>
    <description>We introduce Perceval, an evolutive open-source software platform for simulating and interfacing with discrete variable photonic quantum computers, and describe its main features and components. Its Python front-end allows photonic circuits to be composed from basic photonic building blocks like photon sources, beam splitters, phase shifters and detectors. A variety of computational back-ends are available and optimised for different use-cases. These use state-of-the-art simulation techniques covering both weak simulation, or sampling, and strong simulation. We give examples of Perceval in action by reproducing a variety of photonic experiments and simulating photonic implementations of a range of quantum algorithms, from Grover's and Shor's to examples of quantum machine learning. Perceval is intended to be a useful toolkit both for experimentalists wishing to easily model, design, simulate, or optimise a discrete variable photonic experiment, and for theoreticians wishing to design algorithms and applications for discrete-variable photonic quantum computing platforms.</description>
  </item>

  <item>
    <title>Hamiltonian of mean force in the weak-coupling and high-temperature approximations and refined quantum master equations</title>
    <link>http://arxiv.org/pdf/2204.00599</link>
    <author>Grigorii Timofeev, Anton Trushechkin</author>
    <pubDate>Apr 04 2022</pubDate>
    <description>The Hamiltonian of mean force is a widely used concept to describe the modification of the usual canonical Gibbs state for a quantum system whose coupling strength with the thermal bath is non-negligible. Here we perturbatively derive general approximate expressions for the Hamiltonians of mean force in the weak-coupling approximation and in the high-temperature one. We numerically analyse the accuracy of the corresponding expressions and show that the precision of the Bloch-Redfield equantum master equation can be improved if we replace the original system Hamiltonian by the Hamiltonian of mean force.</description>
  </item>

  <item>
    <title>Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation</title>
    <link>http://arxiv.org/pdf/2204.00570</link>
    <author>Kendrick Shen, Robbie Jones, Ananya Kumar, Sang Michael Xie, Jeff Z. HaoChen, Tengyu Ma, Percy Liang</author>
    <pubDate>Apr 04 2022</pubDate>
    <description>We consider unsupervised domain adaptation (UDA), where labeled data from a source domain (e.g., photographs) and unlabeled data from a target domain (e.g., sketches) are used to learn a classifier for the target domain. Conventional UDA methods (e.g., domain adversarial training) learn domain-invariant features to improve generalization to the target domain. In this paper, we show that contrastive pre-training, which learns features on unlabeled source and target data and then fine-tunes on labeled source data, is competitive with strong UDA methods. However, we find that contrastive pre-training does not learn domain-invariant features, diverging from conventional UDA intuitions. We show theoretically that contrastive pre-training can learn features that vary subtantially across domains but still generalize to the target domain, by disentangling domain and class information. Our results suggest that domain invariance is not necessary for UDA. We empirically validate our theory on benchmark vision datasets.</description>
  </item>

</channel>

</rss>